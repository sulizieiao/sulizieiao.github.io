---
layout: post
title: 网络初始化
date: 2020-04-15 
tag: DL基础
---

**固定初始化**是指将模型参数初始化为一个固定的常数，这意味着所有单元具有相同的初始化状态，所有的神经元都具有相同的输出和更新梯度，并进行完全相同的更新，这种初始化方法使得神经元间不存在非对称性，从而使得模型效果大打折扣。

**预训练初始化**是神经网络初始化的有效方式，比较早期的方法是使用 greedy layerwise auto-encoder 做无监督学习的预训练，经典代表为 Deep Belief Network；而现在更为常见的是有监督的预训练+模型微调。

**随机初始化**是指随机进行参数初始化，但如果不考虑随机初始化的分布则会导致梯度爆炸和梯度消失的问题。

## 





**kaiming初始化**

1. 前向传播的时候, 每一层的卷积计算结果的方差为1.
2. 反向传播的时候, 每一 层的继续往前传的梯度方差为1(因为每层会有两个梯度的计算, 一个用来更新当前层的权重, 一个继续传播, 用于前面层的梯度的计算.)





